# Pytorch_Lightning
PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. Lightning evolves with you as your projects go from idea to paper/production.

## O que são Funções de Ativação?
  As funções de ativação são usadas em redes neurais para introduzir não linearidade em suas camadas.

  Em redes neurais profundas, as funções de ativação permitem que os modelos aprendam padrões complexos e abstrair informações úteis a partir do dados de entrada. Algumas das funções de ativação mais comuns incluem a ReLU (unidade linear retificada), a sigmóide e a tangente hiperbólica.

  A ReLU é uma função de ativação bastante popular, pois tem um bom desempenho e é fácil de implementar. Ela é definida como f(x) = max(0,x), ou seja, é simplesmente o valor máximo entre zero e o valor de entrada x.

  Sigmóide é outra função de ativação comum, que é útil para problemas de classificação binária. Ela é definida como f(x) = 1/(1+e^(-x)).

  A tangente hiperbólica é outra função de ativação que é frenquentemente usada em redes neurais. Ela é definida como f(x) = tanh(x)

  Essas são apenas algumas das muitas funções de ativação disponíveis e casa uma delas pode ser mais adequada para determinados tipos de problemas. É importante experimentar diferentes funções de ativação para ver qual tem o melhor desempenho para o seu problema específico.

  No link abaixo você encontrará uma lista constantemente atualizada de funções de ativação com o ano de lançamento de cada uma, além das principais aplicações e usos:
  https://paperswithcode.com/methods/category/activation-functions

  ![image](https://github.com/WeberSouzaWeb/Pytorch_Lightning/assets/107212929/507113fe-6aa4-4923-90c6-b93f0fbfae8e)
